{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.transform import resize, downscale_local_mean\n",
    "import numpy as np\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "\n",
    "from Utilities.data import DataWrapper\n",
    "from Models.SRGanGenerator import Generator\n",
    "from Models.SRGanDiscriminator import Discriminator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.AdaptiveAvgPool2d(1)\n",
    "a = np.random.random((32,1,64,127))\n",
    "\n",
    "myinput = torch.from_numpy(a)\n",
    "output = m(myinput)\n",
    "\n",
    "#print(output.view(-1).data.numpy().shape)\n",
    "mean1 = output.view(-1).data.numpy()\n",
    "mean2 = np.mean(a.reshape(32,-1),axis=1)\n",
    "\n",
    "#print(mean1)\n",
    "#print(mean2)\n",
    "#print(np.mean((np.abs(mean1-mean2)>0.0001).astype(float)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Conv2d(512,1024,kernel_size=6, padding=0, bias=True)\n",
    "a = np.random.random((32,512,7,7))\n",
    "\n",
    "myinput = Variable(torch.from_numpy(a).float())\n",
    "output = m(myinput)\n",
    "\n",
    "#print(output.data.numpy().shape)\n",
    "myout = output.view(-1).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.__version__.startswith('0.3'):\n",
    "    from torch.nn.modules.module import Module\n",
    "    class BCEWithLogitsLoss(Module):\n",
    "        r\"\"\"This loss combines a `Sigmoid` layer and the `BCELoss` in one single\n",
    "        class. This version is more numerically stable than using a plain `Sigmoid`\n",
    "        followed by a `BCELoss` as, by combining the operations into one layer,\n",
    "        we take advantage of the log-sum-exp trick for numerical stability.\n",
    "\n",
    "        This Binary Cross Entropy between the target and the output logits\n",
    "        (no sigmoid applied) is:\n",
    "\n",
    "        .. math:: loss(o, t) = - 1/n \\sum_i (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\n",
    "\n",
    "        or in the case of the weight argument being specified:\n",
    "\n",
    "        .. math:: loss(o, t) = - 1/n \\sum_i weight[i] * (t[i] * log(sigmoid(o[i])) + (1 - t[i]) * log(1 - sigmoid(o[i])))\n",
    "\n",
    "        This is used for measuring the error of a reconstruction in for example\n",
    "        an auto-encoder. Note that the targets `t[i]` should be numbers\n",
    "        between 0 and 1.\n",
    "\n",
    "        Args:\n",
    "            weight (Tensor, optional): a manual rescaling weight given to the loss\n",
    "                of each batch element. If given, has to be a Tensor of size\n",
    "                \"nbatch\".\n",
    "            size_average (bool, optional): By default, the losses are averaged\n",
    "                over observations for each minibatch. However, if the field\n",
    "                size_average is set to ``False``, the losses are instead summed for\n",
    "                each minibatch. Default: ``True``\n",
    "\n",
    "         Shape:\n",
    "             - Input: :math:`(N, *)` where `*` means, any number of additional\n",
    "               dimensions\n",
    "             - Target: :math:`(N, *)`, same shape as the input\n",
    "\n",
    "         Examples::\n",
    "\n",
    "             >>> loss = nn.BCEWithLogitsLoss()\n",
    "             >>> input = autograd.Variable(torch.randn(3), requires_grad=True)\n",
    "             >>> target = autograd.Variable(torch.FloatTensor(3).random_(2))\n",
    "             >>> output = loss(input, target)\n",
    "             >>> output.backward()\n",
    "        \"\"\"\n",
    "        def __init__(self, weight=None, size_average=True, reduce=True):\n",
    "            super(BCEWithLogitsLoss, self).__init__()\n",
    "            self.size_average = size_average\n",
    "            self.reduce=reduce\n",
    "            self.register_buffer('weight', weight)\n",
    "\n",
    "        def forward(self, input, target):\n",
    "            if self.weight is not None:\n",
    "                if self.reduce:\n",
    "                    return F.binary_cross_entropy_with_logits(input, target, Variable(self.weight), self.size_average).sum()\n",
    "                else:\n",
    "                    return F.binary_cross_entropy_with_logits(input, target, Variable(self.weight), self.size_average)\n",
    "            else:\n",
    "                if self.reduce:\n",
    "                    return F.binary_cross_entropy_with_logits(input, target, size_average=self.size_average).sum()\n",
    "                else:\n",
    "                    return F.binary_cross_entropy_with_logits(input, target, size_average=self.size_average)\n",
    "else:\n",
    "    BCEWithLogitsLoss = nn.BCEWithLogitsLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if torch.__version__.startswith('0.3'):\n",
    "    from torch.nn.modules.module import Module\n",
    "    def _assert_no_grad(variable):\n",
    "        assert not variable.requires_grad, \\\n",
    "            \"nn criterions don't compute the gradient w.r.t. targets - please \" \\\n",
    "            \"mark these variables as volatile or not requiring gradients\"\n",
    "\n",
    "    class _Loss(Module):\n",
    "        def __init__(self, size_average=True, reduce=True):\n",
    "            super(_Loss, self).__init__()\n",
    "            self.size_average = size_average\n",
    "            self.reduce=reduce\n",
    "\n",
    "    class SoftMarginLoss(_Loss):\n",
    "        r\"\"\"Creates a criterion that optimizes a two-class classification\n",
    "        logistic loss between input `x` (a 2D mini-batch Tensor) and\n",
    "        target `y` (which is a tensor containing either `1` or `-1`).\n",
    "\n",
    "        ::\n",
    "\n",
    "            loss(x, y) = sum_i (log(1 + exp(-y[i]*x[i]))) / x.nelement()\n",
    "\n",
    "        The normalization by the number of elements in the input can be disabled by\n",
    "        setting `self.size_average` to ``False``.\n",
    "        \"\"\"\n",
    "        def forward(self, input, target, reduce=True):\n",
    "            _assert_no_grad(target)\n",
    "            if self.reduce:\n",
    "                return F.soft_margin_loss(input, target, size_average=self.size_average).sum()\n",
    "            else:\n",
    "                return F.soft_margin_loss(input, target, size_average=self.size_average)\n",
    "else:\n",
    "    SoftMarginLoss = nn.SoftMarginLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
    "        init.constant(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant(m.weight, 1)\n",
    "        init.constant(m.bias, 0)\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "IS_GPU=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23565505\n",
      "23565505\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "# Create an instance of the nn.module class defined above:\n",
    "net = Discriminator(init_ch_expansion=64, B=4, k=3, fcn_kernel=6, dense_nuerons=[1024])\n",
    "#net = Generator(first_stage_hyperparams={'k':9, 'n':64, 's':1}, \n",
    "#                 residual_blocks_hyperparams={'k':3, 'n':64, 's':1, 'B':16}, \n",
    "#                 upsample_blocks_hyperparams={'k':3, 'n':256, 's':1, 'B':2, 'f':2}, \n",
    "#                 last_stage_hyperparams={'k':9, 's':1} )\n",
    "\n",
    "net.apply(conv_init)\n",
    "\n",
    "# For training on GPU, we need to transfer net and data onto the GPU\n",
    "# http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n",
    "if IS_GPU:\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    net = net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "\n",
    "##For Generator\n",
    "#a =  (81*3 + 1 ) *64 +64\n",
    "#a += ( (9*64 + 1)*64*2 + 5*64)*16 + (9*64 + 1)*64 + 2*64\n",
    "#a += ((9*64 + 1)*256 + 64)*2\n",
    "#a += (81*64+1)*3\n",
    "\n",
    "#For Discriminator\n",
    "a =  (9 * 3   + 1) * 64\n",
    "a += (9 * 64  + 1) * 64  + 2 * 64\n",
    "a += (9 * 64  + 1) * 128 + 2 * 128\n",
    "a += (9 * 128 + 1) * 128 + 2 * 128\n",
    "a += (9 * 128 + 1) * 256 + 2 * 256\n",
    "a += (9 * 256 + 1) * 256 + 2 * 256\n",
    "a += (9 * 256 + 1) * 512 + 2 * 512\n",
    "a += (9 * 512 + 1) * 512 + 2 * 512\n",
    "a += (6 * 6 * 512 + 1) * 1024\n",
    "a += 1024 + 1\n",
    "print(count_parameters(net))\n",
    "print(a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 3. Define a Loss function and optimizer\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Here we use Cross-Entropy loss and SGD with momentum.\n",
    "# The CrossEntropyLoss criterion already includes softmax within its\n",
    "# implementation. That's why we don't use a softmax in our model\n",
    "# definition.\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = nn.BCEWithLogitsLoss(size_average=True)\n",
    "criterion = SoftMarginLoss(size_average=False,reduce=True)\n",
    "\n",
    "# Tune the learning rate.\n",
    "# See whether the momentum is useful or not\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training=DataWrapper()\n",
    "for i_batch , sample_batch in enumerate(training.dataset):\n",
    "    #print((sample_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.957386\n",
      "47.9573860168457\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "net.apply(conv_init)\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training.loader, 0):\n",
    "        # get the inputs\n",
    "        \n",
    "        inputs = data['High']\n",
    "        #labels = data['High']\n",
    "\n",
    "        if IS_GPU:\n",
    "            inputs = inputs.cuda()\n",
    "            #labels = labels.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs)\n",
    "        #labels = Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(inputs.shape)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(outputs.type)\n",
    "        #print(labels.type)\n",
    "        labels = torch.ones_like(outputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #a=(torch.cuda.FloatTensor(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #print(outputs.cpu().data.numpy())\n",
    "\n",
    "        # print statistics\n",
    "        #print(np.mean(np.square(outputs.cpu().data.numpy() - labels.cpu().data.numpy())))\n",
    "        sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "        print(np.sum( -1*np.log(sigmoid(outputs.cpu().data.numpy() * labels.cpu().data.numpy()))) )\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        print(running_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
