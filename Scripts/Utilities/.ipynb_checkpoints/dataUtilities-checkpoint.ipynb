{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.transform import resize, downscale_local_mean\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class HighResolutionDataset(Dataset):\n",
    "    \"\"\"Preparing High Resolution dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images.\n",
    "        \"\"\"\n",
    "        assert os.path.isdir(root_dir), 'Root directory does not exist: '+str(root_dir)\n",
    "        self.root_dir = root_dir\n",
    "        self.imnames=[name for name in os.listdir(self.root_dir) if \n",
    "                      os.path.isfile(os.path.join(self.root_dir, name)) and\n",
    "                     (name.lower().endswith('.png') or name.lower().endswith('.jpg') or\n",
    "                      name.lower().endswith('.bmp') or name.lower().endswith('.tif') or\n",
    "                      name.lower().endswith('.tiff')) ]\n",
    "        assert len(self.imnames)>0, 'No known image types found in the root directory: '+str(root_dir)\n",
    "        self.transform=transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imnames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir,self.imnames[idx])\n",
    "        image = io.imread(img_path)\n",
    "        sample = {'High': image}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RandomCrop(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            assert len(output_size) == 2\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample['High']\n",
    "\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "\n",
    "        image = image[top: top + new_h,\n",
    "                      left: left + new_w]\n",
    "\n",
    "        return {'High': image}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AddLowResolution(object):\n",
    "    \"\"\"Crop randomly the image in a sample.\n",
    "\n",
    "    Args:\n",
    "        scale_factor (tuple or int): Desired downscaling factor. \n",
    "        output_size (tuple or int): Desired output size. If int, square crop\n",
    "            is made.\n",
    "        ****The scale_factor and output_size need to be provided mutually exclusive\n",
    "        ****(i.e. exactly one of them should be presented).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, down_scale_factor=4, output_size=None):\n",
    "        assert isinstance(output_size, (int, tuple)) or isinstance(down_scale_factor, int)\n",
    "        if isinstance(down_scale_factor, (int, tuple)):\n",
    "            self.down_scale_factor = down_scale_factor\n",
    "            if isinstance(down_scale_factor, int):\n",
    "                self.down_scale_factor = (down_scale_factor, down_scale_factor)\n",
    "            self.factor_exist=True\n",
    "        else:\n",
    "            self.factor_exist=False\n",
    "            if isinstance(output_size, int):\n",
    "                self.output_size = (output_size, output_size)\n",
    "            else:\n",
    "                assert len(output_size) == 2\n",
    "                self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        hr_image = sample['High']\n",
    "\n",
    "        h, w = hr_image.shape[:2]\n",
    "        if self.factor_exist:\n",
    "            new_h = int(h/self.down_scale_factor[0])\n",
    "            new_w = int(w/self.down_scale_factor[1])\n",
    "            output_shape = (new_h , new_w)\n",
    "        else:\n",
    "            output_shape = self.output_size\n",
    "            \n",
    "        lr_image = resize(hr_image, output_shape, order=1, mode='reflect', \n",
    "                          clip=True, preserve_range=True)\n",
    "        sample['Low'] = lr_image\n",
    "\n",
    "        return sample\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizeRange(object):\n",
    "    \"\"\"Normalizes the high and low resolution ranges.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_range = (0,255), out_range=(0,1), image_key='Low', out_type = np.float):\n",
    "        assert isinstance(out_range, tuple)\n",
    "        assert isinstance(in_range, tuple)\n",
    "        assert len(out_range) == 2 \n",
    "        assert len(in_range) == 2\n",
    "        \n",
    "        self.out_min = np.float(out_range[0])\n",
    "        self.out_max = np.float(out_range[1])\n",
    "        \n",
    "        self.in_min = np.float(in_range[0])\n",
    "        self.in_max = np.float(in_range[1])\n",
    "        \n",
    "        #Output = m * Input + b\n",
    "        self.m = (self.out_max - self.out_min)/(self.in_max - self.in_min)\n",
    "        self.b = self.out_min - self.m * self.in_min\n",
    "        \n",
    "        self.image_key = image_key\n",
    "        self.out_type = out_type\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image = sample[self.image_key]\n",
    "\n",
    "        out_image = image.astype(np.float) * self.m + self.b\n",
    "        out_image = out_image.astype(self.out_type)\n",
    "        \n",
    "        sample[self.image_key] = image\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        \n",
    "        for curr_attr in sample.keys():\n",
    "            \n",
    "            curr_val=sample[curr_attr]\n",
    "            \n",
    "            if curr_attr=='High' or curr_attr=='Low':\n",
    "                # swap color axis because\n",
    "                # numpy image: H x W x C\n",
    "                # torch image: C X H X W\n",
    "                sample[curr_attr] = torch.from_numpy( curr_val.transpose((2, 0, 1)) )\n",
    "            else:\n",
    "                sample[curr_attr] = torch.from_numpy( curr_val )\n",
    "                \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataWrapper(object):\n",
    "    \"\"\"Creates a dataset\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, batch_size=32, random_crop_size=96, down_scale_factor=4, \n",
    "                 root_dir='/u/sciteam/saleh1/work/srgan/Data/HighResolution',\n",
    "                 loader_shuffle=True, loader_workers=4):\n",
    "        self.batch_size = batch_size\n",
    "        self.root_dir=root_dir\n",
    "        self.loader_shuffle=loader_shuffle\n",
    "        self.loader_workers=loader_workers\n",
    "        \n",
    "        transform_list=[]\n",
    "        if random_crop_size:\n",
    "            transform_list.append(RandomCrop(random_crop_size))\n",
    "        \n",
    "        transform_list.append(AddLowResolution(down_scale_factor=down_scale_factor))\n",
    "        \n",
    "        #transform_list.append(NormalizeRange(in_range = (0,255), out_range=(0,1),\n",
    "        #                                     image_key='Low', out_type = np.float))\n",
    "        \n",
    "        #transform_list.append(NormalizeRange(in_range = (0,255), out_range=(-1,1),\n",
    "        #                                     image_key='High', out_type = np.float))\n",
    "        \n",
    "        transform_list.append(ToTensor())\n",
    "        \n",
    "        self.full_transform = transforms.Compose(transform_list)\n",
    "        \n",
    "        \n",
    "        self.dataset = HighResolutionDataset(root_dir=self.root_dir, transform = self.full_transform)\n",
    "        \n",
    "        self.loader = torch.utils.data.DataLoader(self.dataset,\n",
    "                                                  batch_size=self.batch_size,\n",
    "                                                  shuffle=self.loader_shuffle,\n",
    "                                                  num_workers=self.loader_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training=DataWrapper()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'High': \n",
      "(0 ,.,.) = \n",
      "  188  181  174  ...   255  254  255\n",
      "  191  172  170  ...   255  254  255\n",
      "  187  173  167  ...   255  254  255\n",
      "      ...         ⋱        ...      \n",
      "  194  200  201  ...   199  203  201\n",
      "  190  192  196  ...   197  202  203\n",
      "  186  187  180  ...   199  201  198\n",
      "\n",
      "(1 ,.,.) = \n",
      "  149  143  137  ...   236  231  235\n",
      "  150  129  127  ...   235  233  235\n",
      "  146  131  124  ...   235  233  235\n",
      "      ...         ⋱        ...      \n",
      "  150  151  152  ...   155  155  152\n",
      "  145  146  149  ...   156  156  156\n",
      "  140  139  130  ...   156  158  153\n",
      "\n",
      "(2 ,.,.) = \n",
      "  107  100   93  ...   201  193  192\n",
      "  103   80   80  ...   199  196  195\n",
      "   98   84   78  ...   199  197  194\n",
      "      ...         ⋱        ...      \n",
      "   98   98   99  ...   113  113  109\n",
      "   92   90   94  ...   115  114  114\n",
      "   82   80   75  ...   120  119  115\n",
      "[torch.ByteTensor of size 3x96x96]\n",
      ", 'Low': \n",
      "(0 ,.,.) = \n",
      "  0.6686  0.6676  0.6980  ...   0.7392  0.8637  0.9980\n",
      "  0.6667  0.6686  0.7294  ...   0.7324  0.7333  0.9980\n",
      "  0.7186  0.7265  0.7108  ...   0.7304  0.7127  0.9961\n",
      "           ...             ⋱             ...          \n",
      "  0.7343  0.6657  0.7637  ...   0.7686  0.7833  0.6951\n",
      "  0.7627  0.7412  0.7853  ...   0.8108  0.8098  0.7225\n",
      "  0.7735  0.7794  0.7667  ...   0.8265  0.8225  0.7853\n",
      "\n",
      "(1 ,.,.) = \n",
      "  0.5010  0.4892  0.5275  ...   0.5569  0.7382  0.9176\n",
      "  0.5078  0.4971  0.5686  ...   0.5245  0.5676  0.9118\n",
      "  0.5559  0.5637  0.5559  ...   0.5255  0.5167  0.8902\n",
      "           ...             ⋱             ...          \n",
      "  0.5588  0.4794  0.5843  ...   0.5529  0.5980  0.4500\n",
      "  0.5941  0.5578  0.5951  ...   0.6265  0.6333  0.4990\n",
      "  0.5863  0.6020  0.5716  ...   0.6402  0.6392  0.6098\n",
      "\n",
      "(2 ,.,.) = \n",
      "  0.3157  0.3069  0.3363  ...   0.4196  0.5765  0.7755\n",
      "  0.3294  0.3000  0.3892  ...   0.3922  0.4275  0.7529\n",
      "  0.3667  0.3765  0.3706  ...   0.4039  0.3863  0.7284\n",
      "           ...             ⋱             ...          \n",
      "  0.3618  0.2578  0.3696  ...   0.4078  0.4520  0.3137\n",
      "  0.3824  0.3422  0.3833  ...   0.4578  0.4725  0.3588\n",
      "  0.3735  0.3941  0.3735  ...   0.4667  0.4696  0.4461\n",
      "[torch.DoubleTensor of size 3x24x24]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for i_batch , sample_batch in enumerate(training.dataset):\n",
    "    print((sample_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 512)\n",
      "(100, 100)\n"
     ]
    }
   ],
   "source": [
    "from skimage import data\n",
    "from skimage.transform import resize\n",
    "image = data.camera()\n",
    "print(image.shape)\n",
    "print(resize(image, (100, 100), mode='reflect').shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=np.ones(10,np.float64)\n",
    "np.issubdtype(a.dtype, np.integer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 5, 3: 4}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a={1:2,3:4}\n",
    "b=a\n",
    "a[1]=5\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
