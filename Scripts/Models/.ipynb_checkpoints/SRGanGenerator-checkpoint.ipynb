{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "from skimage import io\n",
    "from skimage.transform import resize, downscale_local_mean\n",
    "import numpy as np\n",
    "from Utilities.data import DataWrapper\n",
    "\n",
    "\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenResLego(nn.Module):\n",
    "    def __init__(self, inp_channels=64, n=64, k=3, s=1):\n",
    "        super(GenResLego, self).__init__()\n",
    "        p = int(k/2)\n",
    "        self.conv1 = nn.Conv2d(inp_channels, n, kernel_size=k, padding=p, stride=s, bias=True)\n",
    "        self.bn1 = nn.BatchNorm2d(n)\n",
    "        self.prelu = nn.PReLU(n)\n",
    "        self.conv2 = nn.Conv2d(n, n, kernel_size=k, padding=1, stride=s, bias=True)\n",
    "        self.bn2 = nn.BatchNorm2d(n)\n",
    "                \n",
    "    def forward(self,x):\n",
    "        y = self.prelu(self.bn1(self.conv1(x)))\n",
    "        z = self.bn2(self.conv2(y))\n",
    "        out = z + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenResBlock(nn.Module):\n",
    "    def __init__(self, inp_channels=64, B=16, n=64, k=3, s=1):\n",
    "        super(GenResBlock, self).__init__()\n",
    "        \n",
    "        assert B>=1\n",
    "        \n",
    "        self.building_Blocks = nn.Sequential()\n",
    "        self.building_Blocks.add_module('Residual_Block_0' , GenResLego(inp_channels=inp_channels, \n",
    "                                                                        n=n, k=k, s=s))\n",
    "        for b in range(B-1):\n",
    "            self.building_Blocks.add_module('Residual_Block_' + str(b+1) , \n",
    "                                            GenResLego(inp_channels=n, n=n, k=k, s=s))\n",
    "        p = int(k/2)    \n",
    "        self.final_conv = nn.Conv2d(n, n, kernel_size=k, padding=p, stride=s, bias=True)\n",
    "        self.final_bn = nn.BatchNorm2d(n)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.building_Blocks.forward(x)\n",
    "        z = self.final_conv(y)\n",
    "        out = self.final_bn(z) + x\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class GenUpsampleLego(nn.Module):\n",
    "    def __init__(self, inp_channels=64, n=256, k=3, s=1, upscale_factor=2):\n",
    "        super(GenUpsampleLego, self).__init__()\n",
    "        \n",
    "        p = int(k/2)\n",
    "        self.conv = nn.Conv2d(inp_channels, n, kernel_size=k, padding=p, stride=s, bias=True)\n",
    "        self.upsamp = nn.PixelShuffle(upscale_factor = upscale_factor)\n",
    "        self.prelu = nn.PReLU(inp_channels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.conv(x)\n",
    "        z = self.upsamp(y)\n",
    "        out = self.prelu(z)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenUpsampleBlock(nn.Module):\n",
    "    def __init__(self, upsample_B=2, inp_channels=64, n=256, k=3, s=1, upscale_factor=2):\n",
    "        super(GenUpsampleBlock, self).__init__()\n",
    "        \n",
    "        errorlog ='The upscale factor controls the pixel shuffler and changes then number of channels. ' \n",
    "        errorlog +='Make sure that you choose the hyperparams in a way that n == upscale_factor^2 * inp_channels.'\n",
    "        assert n == upscale_factor * upscale_factor * inp_channels ,  errorlog\n",
    "        \n",
    "        \n",
    "        self.building_Blocks = nn.Sequential()\n",
    "        for b in range(upsample_B):\n",
    "            self.building_Blocks.add_module('Upsample_Block_' + str(b) ,\n",
    "                                            GenUpsampleLego(inp_channels=inp_channels, n=n,\n",
    "                                                            k=k, s=s, upscale_factor=upscale_factor))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.building_Blocks.forward(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, first_stage_hyperparams={'k':9, 'n':64, 's':1}, \n",
    "                 residual_blocks_hyperparams={'k':3, 'n':64, 's':1, 'B':16}, \n",
    "                 upsample_blocks_hyperparams={'k':3, 'n':256, 's':1, 'B':2, 'f':2}, \n",
    "                 last_stage_hyperparams={'k':9, 's':1} ):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        fsh = first_stage_hyperparams\n",
    "        rbh = residual_blocks_hyperparams\n",
    "        ubh = upsample_blocks_hyperparams\n",
    "        lsh = last_stage_hyperparams\n",
    "        \n",
    "        fsh['p']=int(fsh['k']/2)\n",
    "        lsh['p']=int(lsh['k']/2)\n",
    "        self.first_stage_conv = nn.Conv2d(3, fsh['n'], kernel_size=fsh['k'], \n",
    "                                          padding=fsh['p'], stride=fsh['s'], bias=True)\n",
    "        self.first_stage_prelu = nn.PReLU(fsh['n'])\n",
    "        self.ResBlocks = GenResBlock(inp_channels=fsh['n'], n=rbh['n'], k=rbh['k'], s=rbh['s'], B=rbh['B'])\n",
    "        self.UpscaleBlocks = GenUpsampleBlock(upsample_B=ubh['B'], inp_channels=rbh['n'], n=ubh['n'], k=ubh['k'], \n",
    "                                              s=ubh['s'], upscale_factor=ubh['f'])\n",
    "        self.last_stage_conv = nn.Conv2d(rbh['n'], 3, kernel_size=lsh['k'], \n",
    "                                          padding=lsh['p'], stride=lsh['s'], bias=True)\n",
    "        \n",
    "        \n",
    "    def forward(self,x):\n",
    "        y = self.first_stage_conv(x)\n",
    "        z = self.first_stage_prelu(y)\n",
    "        u = self.ResBlocks(z)\n",
    "        v = self.UpscaleBlocks(u)\n",
    "        out = self.last_stage_conv(v)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1550659\n",
      "1550659\n"
     ]
    }
   ],
   "source": [
    "IS_GPU=True\n",
    "import torch.nn.init as init\n",
    "\n",
    "def conv_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        init.xavier_uniform(m.weight, gain=np.sqrt(2))\n",
    "        init.constant(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        init.constant(m.weight, 1)\n",
    "        init.constant(m.bias, 0)\n",
    "    \n",
    "# Create an instance of the nn.module class defined above:\n",
    "net = Generator(first_stage_hyperparams={'k':9, 'n':64, 's':1}, \n",
    "                 residual_blocks_hyperparams={'k':3, 'n':64, 's':1, 'B':16}, \n",
    "                 upsample_blocks_hyperparams={'k':3, 'n':256, 's':1, 'B':2, 'f':2}, \n",
    "                 last_stage_hyperparams={'k':9, 's':1} )\n",
    "\n",
    "net.apply(conv_init)\n",
    "\n",
    "# For training on GPU, we need to transfer net and data onto the GPU\n",
    "# http://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#training-on-gpu\n",
    "if IS_GPU:\n",
    "    import torch.backends.cudnn as cudnn\n",
    "    net = net.cuda()\n",
    "    net = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(net))\n",
    "\n",
    "a =  (81*3 + 1 ) *64 +64\n",
    "a += ( (9*64 + 1)*64*2 + 5*64)*16 + (9*64 + 1)*64 + 2*64\n",
    "a += ((9*64 + 1)*256 + 64)*2\n",
    "a += (81*64+1)*3\n",
    "print(a)\n",
    "\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# 3. Define a Loss function and optimizer\n",
    "# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "# Here we use Cross-Entropy loss and SGD with momentum.\n",
    "# The CrossEntropyLoss criterion already includes softmax within its\n",
    "# implementation. That's why we don't use a softmax in our model\n",
    "# definition.\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Tune the learning rate.\n",
    "# See whether the momentum is useful or not\n",
    "#optimizer = optim.SGD(net.parameters(), lr=0.005, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training=DataWrapper()\n",
    "for i_batch , sample_batch in enumerate(training.dataset):\n",
    "    #print((sample_batch))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4342948\n",
      "1.434294581413269\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=1\n",
    "for epoch in range(EPOCHS):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(training.loader, 0):\n",
    "        # get the inputs\n",
    "        \n",
    "        inputs = data['Low']\n",
    "        labels = data['High']\n",
    "\n",
    "        if IS_GPU:\n",
    "            inputs = inputs.cuda()\n",
    "            labels = labels.cuda()\n",
    "\n",
    "        # wrap them in Variable\n",
    "        inputs = Variable(inputs)\n",
    "        labels = Variable(labels)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        #print(inputs.shape)\n",
    "        #print(outputs.shape)\n",
    "        #print(labels.shape)\n",
    "        #print(outputs.type)\n",
    "        #print(labels.type)\n",
    "        loss = criterion(outputs, labels)\n",
    "        #a=(torch.cuda.FloatTensor(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        print(np.mean(np.square(outputs.cpu().data.numpy() - labels.cpu().data.numpy())))\n",
    "        running_loss += loss.data[0]\n",
    "        \n",
    "        print(running_loss)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
